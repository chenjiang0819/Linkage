{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ....\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import Series, DataFrame\n",
    "%matplotlib inline\n",
    "\n",
    "#VAT algorithm provided for your use\n",
    "def VAT(R):\n",
    "    \"\"\"\n",
    "\n",
    "    VAT algorithm adapted from matlab version:\n",
    "    http://www.ece.mtu.edu/~thavens/code/VAT.m\n",
    "\n",
    "    Args:\n",
    "        R (n*n double): Dissimilarity data input\n",
    "        R (n*D double): vector input (R is converted to sq. Euclidean distance)\n",
    "    Returns:\n",
    "        RV (n*n double): VAT-reordered dissimilarity data\n",
    "        C (n int): Connection indexes of MST in [0,n)\n",
    "        I (n int): Reordered indexes of R, the input data in [0,n)\n",
    "    \"\"\"\n",
    "        \n",
    "    R = np.array(R)\n",
    "    N, M = R.shape\n",
    "    if N != M:\n",
    "        R = squareform(pdist(R))\n",
    "        \n",
    "    J = list(range(0, N))\n",
    "    \n",
    "    y = np.max(R, axis=0)\n",
    "    i = np.argmax(R, axis=0)\n",
    "    j = np.argmax(y)\n",
    "    y = np.max(y)\n",
    "\n",
    "\n",
    "    I = i[j]\n",
    "    del J[I]\n",
    "\n",
    "    y = np.min(R[I,J], axis=0)\n",
    "    j = np.argmin(R[I,J], axis=0)\n",
    "    \n",
    "    I = [I, J[j]]\n",
    "    J = [e for e in J if e != J[j]]\n",
    "    \n",
    "    C = [1,1]\n",
    "    for r in range(2, N-1):   \n",
    "        y = np.min(R[I,:][:,J], axis=0)\n",
    "        i = np.argmin(R[I,:][:,J], axis=0)\n",
    "        j = np.argmin(y)        \n",
    "        y = np.min(y)      \n",
    "        I.extend([J[j]])\n",
    "        J = [e for e in J if e != J[j]]\n",
    "        C.extend([i[j]])\n",
    "    \n",
    "    y = np.min(R[I,:][:,J], axis=0)\n",
    "    i = np.argmin(R[I,:][:,J], axis=0)\n",
    "    \n",
    "    I.extend(J)\n",
    "    C.extend(i)\n",
    "    \n",
    "    RI = list(range(N))\n",
    "    for idx, val in enumerate(I):\n",
    "        RI[val] = idx\n",
    "\n",
    "    RV = R[I,:][:,I]\n",
    "    \n",
    "    return RV.tolist(), C, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1 Data Linkage:\n",
    "#Naive data linkage without blocking\n",
    "#1.\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "#Read the data \n",
    "googleS = pd.read_csv('google_small.csv',encoding = 'ISO-8859-1') \n",
    "AmazonS = pd.read_csv('amazon_small.csv',encoding = 'ISO-8859-1') \n",
    "amazon_google_truth_small = pd.read_csv('amazon_google_truth_small.csv',encoding = 'ISO-8859-1') \n",
    "\n",
    "# I consider that if there is any chance that Id are similar in the two documents.\n",
    "# Thereforethe, the part of crawler codes were implement on the google_small.csv in order to retrieve id from idGoogleBase\n",
    "session = HTMLSession()\n",
    "size1 = googleS.shape[0]\n",
    "for i in range(size1):\n",
    "    url = googleS.loc[i,'idGoogleBase']\n",
    "#    r = session.get(url)\n",
    "#    print(r.html.text)\n",
    "# Due to broken weblink , we have to use other atrributes to compare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:1.0\n",
      "precision:0.35384615384615387\n"
     ]
    }
   ],
   "source": [
    "#Part1 Data Linkage:\n",
    "#Naive data linkage without blocking\n",
    "#1.\n",
    "\n",
    "import textdistance as td\n",
    "result = {}\n",
    "resultvalue = {}\n",
    "#Set threshold as 0.5, which means that the result will be identified as ture if the result bigger than 0.5 \n",
    "threshold = 0.5\n",
    "#Literate all data in the AmazonS.\n",
    "#The index is no future use, which means after for loop we discard it. Therefore, set _ as index. \n",
    "for _ , amazon in AmazonS.iterrows():\n",
    "# Initialise the maxvalue to -1.0, whcih is the lowest value of normalized_similarity\n",
    "    maxvalue = -1.0\n",
    "# Set a variable to delivery values\n",
    "    target = \"\"\n",
    "#literate all data in the AmazonS.\n",
    "    for _ , google in googleS.iterrows():\n",
    "        # calculate the similarity by hamming function.\n",
    "        name = td.hamming.normalized_similarity(str(amazon['title']), str(google['name']))\n",
    "        desciption = td.hamming.normalized_similarity(str(amazon['description']), str(google['description']))\n",
    "        manufacturer = td.hamming.normalized_similarity(str(amazon['manufacturer']), str(google['manufacturer']))\n",
    "        price = td.hamming.normalized_similarity(str(amazon['price']), str(google['price']))\n",
    "        # sum all value\n",
    "        #After checking the data, we found that the the \"manufacturer\" in the googleS is almost empty.\n",
    "        #However, from where I standing, the \"manufacturer\" is a very impoartant attribute.\n",
    "        #Thus, we keep this attribute but reduce its weight in the sum.\n",
    "        #Also, there is not too much sense to compare the string of price.\n",
    "        #Therefore, we reduce its weight in the sum. \n",
    "        totalvalue = name + desciption + 0.7*manufacturer + 0.3 * price\n",
    "        #Pick the maximum value\n",
    "        if totalvalue > maxvalue:\n",
    "            maxvalue = totalvalue\n",
    "            target = google['idGoogleBase']\n",
    "    #Record the maximum value\n",
    "    resultvalue[amazon['idAmazon']] = maxvalue\n",
    "    result[amazon['idAmazon']] = target\n",
    "\n",
    "#Initialise all tp,tn,fp,fn to 0.    \n",
    "truepositive = 0\n",
    "truenegative = 0\n",
    "falsepositive = 0\n",
    "falsenegative = 0\n",
    "\n",
    "#Calculate tp,tn,fp,fn\n",
    "for _ , truth in amazon_google_truth_small.iterrows():\n",
    "#If value is on the amazon_google_truth_small \n",
    "    if truth['idGoogleBase'] == result[truth['idAmazon']]:\n",
    "#Verify if the maxvalue is bigger than threshold (0.5)\n",
    "        if resultvalue[amazon['idAmazon']] >= threshold:\n",
    "#If the value is bigger than threshold and on the amazon_google_truth_small list it is tp\n",
    "            truepositive += 1  \n",
    "        else:\n",
    "#If the value is smaller than threshold and on the amazon_google_truth_small list it is tn \n",
    "            truenegative += 1\n",
    "\n",
    "#If value is not on the amazon_google_truth_small\n",
    "    else:\n",
    "#To chose the maxvalue that is bigger than threshold (0.5)\n",
    "        if resultvalue[amazon['idAmazon']] >= threshold:\n",
    "#If the value is bigger than threshold and is not on the amazon_google_truth_small list it is tp\n",
    "            falsepositive += 1\n",
    "        else:\n",
    "#If the value is smaller than threshold and is not on the amazon_google_truth_small list it is tp\n",
    "            falsenegative += 1\n",
    "            \n",
    "\n",
    "#Calculate recall and precision\n",
    "recall = truepositive / (truepositive + falsenegative)\n",
    "precision = truepositive / (truepositive + falsepositive)\n",
    "\n",
    "#print out\n",
    "print(\"recall:\" + str(recall))\n",
    "print(\"precision:\" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair completeness:0.8337825696316262\n",
      "Reduction ratio:8.54049023337744e-05\n"
     ]
    }
   ],
   "source": [
    "#Part1 Data Linkage:\n",
    "#Blocking for efficient data linkage\n",
    "#1.\n",
    "\n",
    "#Read data\n",
    "amazonB = pd.read_csv('amazon.csv',encoding = 'ISO-8859-1')\n",
    "googleB = pd.read_csv('google.csv',encoding = 'ISO-8859-1')\n",
    "amazon_google_truthB = pd.read_csv('amazon_google_truth.csv',encoding = 'ISO-8859-1')\n",
    "\n",
    "#Wash data first\n",
    "#In this case we put median value instand of the missing value\n",
    "amazonB.fillna(amazonB.median())\n",
    "googleB.fillna(googleB.median())\n",
    "\n",
    "#Calculate block information\n",
    "#We have around 1000 blocks and averagely we put around 10 data in a block to compare\n",
    "blocknum = 1000.0\n",
    "#Find the smallest and the largest value between amazon data set and google data set\n",
    "maxvalue = max(float(amazonB['price'].max()), float(googleB['price'].max()))\n",
    "minvalue = min(float(amazonB['price'].min()), float(googleB['price'].min()))\n",
    "#Calculate the size of blocks, whcih is 203.0311\n",
    "blocksize = (maxvalue - minvalue) / blocknum\n",
    "#print(blocksize)\n",
    "\n",
    "#Initialise a dictionary to store the block and its content\n",
    "allword = {}\n",
    "\n",
    "#Go through the amazon data\n",
    "for _ , amazon in amazonB.iterrows():\n",
    "    #Choosing the block for a Amazon data\n",
    "    index = int((amazon['price'] - minvalue)/blocksize)\n",
    "#     index = int((amazon['price'] - minvalue)-blocksize)\n",
    "#     index = int((amazon['price'] - minvalue)%blocksize)\n",
    "#     index = int((amazon['price'] - minvalue))\n",
    "    #If we do not have a dictionary in that block we create one \n",
    "    if index not in allword:\n",
    "        allword[index] = dict({\"amazon\": [], \"google\": []})\n",
    "    #Put data in the blocks\n",
    "    allword[index][\"amazon\"].append(amazon['idAmazon'])\n",
    "\n",
    "#Go through the google data\n",
    "for _ , google in googleB.iterrows():\n",
    "    #Choosing the block for a google data\n",
    "    #In the practice, we found some string in the price column. Therefore, we use splite()[0] to remove it\n",
    "    index = int((float(google['price'].split()[0]) - minvalue)/blocksize)\n",
    "#     index = int((float(google['price'].split()[0]) - minvalue)-blocksize)\n",
    "#     index = int((float(google['price'].split()[0]) - minvalue)%blocksize)\n",
    "#     index = int((float(google['price'].split()[0]) - minvalue))\n",
    "    #If we do not have a dictionary in that block we create one  \n",
    "    if index not in allword:\n",
    "        allword[index] = dict({\"amazon\": [], \"google\": []})\n",
    "    #Put data in the blocks\n",
    "    allword[index][\"google\"].append(google['id'])\n",
    "\n",
    "#Generate ground truth pair\n",
    "groundtruth = {}\n",
    "for _ , truth in amazon_google_truthB.iterrows():\n",
    "    groundtruth[truth['idAmazon']] = truth['idGoogleBase']\n",
    "    \n",
    "#Initialise all tp,tn,fp,fn to 0.  \n",
    "truepositive = 0\n",
    "truenegative = 0\n",
    "falsepositive = 0\n",
    "falsenegative = 0\n",
    "recordpair = {}\n",
    "\n",
    "#Go through the all values of allword\n",
    "for value in allword.values():\n",
    "    for amazon in value['amazon']:\n",
    "        for google in value['google']:\n",
    "            #If two data is paired, we record it\n",
    "            recordpair[amazon + google] = True\n",
    "            #Find out the true positive and false positive\n",
    "            if amazon in groundtruth and groundtruth[amazon] == google:\n",
    "                truepositive += 1\n",
    "            if amazon not in groundtruth or groundtruth[amazon] != google:\n",
    "                falsepositive += 1\n",
    "# Compare the record and groundtruth. If it is not in groundtruth it is false negative.\n",
    "for key in groundtruth:\n",
    "    if key + groundtruth[key] not in recordpair:\n",
    "        falsenegative += 1\n",
    "\n",
    "#Calculate Pair completeness\n",
    "pc = truepositive / (truepositive + falsenegative)\n",
    "#Calculatel Reduction ratio\n",
    "rr = 1 - (truepositive + falsepositive) / (truepositive + truenegative + falsepositive + falsenegative)\n",
    "\n",
    "#print out\n",
    "print(\"Pair completeness:\" + str(pc))\n",
    "print(\"Reduction ratio:\" + str(rr))\n",
    "\n",
    "#We also can use the \n",
    "# import re\n",
    "# from datasketch import MinHash, MinHashLSH\n",
    "# #Wash data first\n",
    "# #From my point of view, although some rows have the empty data, they still have the value for comparison. Therefore, instand of\n",
    "# #useing dropna() to remove it we just transform it in the one form which is ' '.\n",
    "# def washing(string):\n",
    "#     #Remove Non-alphabetic(not a-z and not A-Z) characters\n",
    "#     string = re.sub('[^a-zA-Z]', ' ', str(string))\n",
    "#     # split the string according to the space which includes \\n\n",
    "#     return string.split(' ')\n",
    "\n",
    "# #Read data\n",
    "# amazonB = pd.read_csv('amazon.csv',encoding = 'ISO-8859-1')\n",
    "# googleB = pd.read_csv('google.csv',encoding = 'ISO-8859-1')\n",
    "# amazon_google_truthB = pd.read_csv('amazon_google_truth.csv',encoding = 'ISO-8859-1')\n",
    "\n",
    "# #As lecturer mentioned in the class, we use the minhash to hash the data  \n",
    "# #Calculate the amazon minhash\n",
    "# amaz = {}\n",
    "# for _ , amazon in amazonB.iterrows():\n",
    "#     # num_perm for hash key length\n",
    "#     tmp = MinHash(num_perm=128)\n",
    "#     for item in washing(amazon['title']):\n",
    "#         tmp.update(item.encode('ISO-8859-1'))\n",
    "#     for item in washing(amazon['manufacturer']):\n",
    "#         tmp.update(item.encode('ISO-8859-1'))\n",
    "#     for item in washing(amazon['price']):\n",
    "#         tmp.update(item.encode('ISO-8859-1'))\n",
    "#     amaz[amazon['idAmazon']] = tmp\n",
    "    \n",
    "# #Calculate the google minhash    \n",
    "# goog = {}\n",
    "# # cal google minhash\n",
    "# for _ , google in googleB.iterrows():\n",
    "#     # num_perm for hash key length\n",
    "#     tmp = MinHash(num_perm=128)\n",
    "#     for item in washing(google['name']):\n",
    "#         tmp.update(item.encode('ISO-8859-1'))\n",
    "#     for item in washing(google['manufacturer']):\n",
    "#         tmp.update(item.encode('ISO-8859-1'))\n",
    "#     tmp.update(str(google['price']).encode('ISO-8859-1'))\n",
    "#     goog[google['id']] = tmp\n",
    "\n",
    "# #Initialise the sensitive hash functions (LSH)\n",
    "# lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "# #Add google data into LSH\n",
    "# for key in goog:\n",
    "#     lsh.insert(key, goog[key])\n",
    "\n",
    "# #Initialise all tp,tn,fp,fn to 0.   \n",
    "# truepositive2 = 0\n",
    "# truenegative2 = 0\n",
    "# falsepositive2 = 0\n",
    "# falsenegative2 = 0\n",
    "\n",
    "# #Calculate tp,tn,fp,fn\n",
    "# for _ , truth in amazon_google_truthB.iterrows():\n",
    "#     #Get the result of amazon\n",
    "#     outputresult = lsh.query(amaz[truth['idAmazon']])\n",
    "#     predictresult = []\n",
    "#     if len(outputresult) > 0:\n",
    "#         predictresult.append(outputresult[0])\n",
    "#     if len(truth['idGoogleBase']) != 0 and len(predictresult) == 0:\n",
    "#         falsenegative2 += 1\n",
    "#     elif len(truth['idGoogleBase']) == 0 and len(predictresult) == 0:\n",
    "#         truenegative2 += 1\n",
    "#     elif truth['idGoogleBase'] == predictresult[0]:\n",
    "#         truepositive2 += 1\n",
    "#     elif truth['idGoogleBase'] != predictresult[0]:\n",
    "#         falsepositive2 += 1\n",
    "        \n",
    "# #Calculate Pair completeness and Reduction ratio\n",
    "# pc = truepositive2 / (truepositive2 + falsenegative2)\n",
    "# rr = 1 - (truepositive2 + falsepositive2) / (truepositive2 + truenegative2 + falsepositive2 + falsenegative2)\n",
    "\n",
    "# #print out\n",
    "# print(\"Pair completeness:\" + str(pc))\n",
    "# print(\"Reduction ratio:\" + str(rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part2 Classification:\n",
    "#Pre-processing\n",
    "#Impute missing values\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "#Set the printing function\n",
    "def displaydata(df):\n",
    "    print(\"Min:\")\n",
    "    print(df.min())\n",
    "    print(\"Median\")\n",
    "    print(df.median())\n",
    "    print(\"Max\")\n",
    "    print(df.max())\n",
    "    print(\"Mean\")\n",
    "    print(df.mean())\n",
    "    print(\"Standard Deviation\")\n",
    "    print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Display the min, median, max, mean and standard deviation for the data with Mean imputations -------\n",
      "Min:\n",
      "mcg      0.11\n",
      "gvh      0.13\n",
      "alm      0.21\n",
      "mit      0.00\n",
      "erl      0.50\n",
      "pox      0.00\n",
      "vac      0.00\n",
      "nuc      0.00\n",
      "Class    0.00\n",
      "dtype: float64\n",
      "Median\n",
      "mcg      0.499349\n",
      "gvh      0.490000\n",
      "alm      0.510000\n",
      "mit      0.230000\n",
      "erl      0.500000\n",
      "pox      0.000000\n",
      "vac      0.510000\n",
      "nuc      0.220000\n",
      "Class    0.000000\n",
      "dtype: float64\n",
      "Max\n",
      "mcg      1.000000\n",
      "gvh      1.000000\n",
      "alm      7.501819\n",
      "mit      3.000728\n",
      "erl      3.500849\n",
      "pox      0.830000\n",
      "vac      6.001456\n",
      "nuc      4.501092\n",
      "Class    1.000000\n",
      "dtype: float64\n",
      "Mean\n",
      "mcg      0.499349\n",
      "gvh      0.499876\n",
      "alm      0.505848\n",
      "mit      0.264379\n",
      "erl      0.506921\n",
      "pox      0.007500\n",
      "vac      0.503816\n",
      "nuc      0.279816\n",
      "Class    0.311995\n",
      "dtype: float64\n",
      "Standard Deviation\n",
      "mcg      0.131357\n",
      "gvh      0.121945\n",
      "alm      0.199837\n",
      "mit      0.150286\n",
      "erl      0.091573\n",
      "pox      0.075683\n",
      "vac      0.153481\n",
      "nuc      0.151736\n",
      "Class    0.463464\n",
      "dtype: float64\n",
      "------- Display the min, median, max, mean and standard deviation for the data with Median imputations -------\n",
      "Min:\n",
      "mcg      0.11\n",
      "gvh      0.13\n",
      "alm      0.21\n",
      "mit      0.00\n",
      "erl      0.50\n",
      "pox      0.00\n",
      "vac      0.00\n",
      "nuc      0.00\n",
      "Class    0.00\n",
      "dtype: float64\n",
      "Median\n",
      "mcg      0.48\n",
      "gvh      0.49\n",
      "alm      0.51\n",
      "mit      0.22\n",
      "erl      0.50\n",
      "pox      0.00\n",
      "vac      0.51\n",
      "nuc      0.22\n",
      "Class    0.00\n",
      "dtype: float64\n",
      "Max\n",
      "mcg      1.000000\n",
      "gvh      1.000000\n",
      "alm      7.501819\n",
      "mit      3.000728\n",
      "erl      3.500849\n",
      "pox      0.830000\n",
      "vac      6.001456\n",
      "nuc      4.501092\n",
      "Class    1.000000\n",
      "dtype: float64\n",
      "Mean\n",
      "mcg      0.497628\n",
      "gvh      0.499643\n",
      "alm      0.506167\n",
      "mit      0.260432\n",
      "erl      0.506739\n",
      "pox      0.007500\n",
      "vac      0.504199\n",
      "nuc      0.276712\n",
      "Class    0.311995\n",
      "dtype: float64\n",
      "Standard Deviation\n",
      "mcg      0.131472\n",
      "gvh      0.121954\n",
      "alm      0.199840\n",
      "mit      0.150817\n",
      "erl      0.091580\n",
      "pox      0.075683\n",
      "vac      0.153488\n",
      "nuc      0.152315\n",
      "Class    0.463464\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Part2 Classification:\n",
    "#Pre-processing\n",
    "#Impute missing values\n",
    "\n",
    "#Read data\n",
    "yeast = pd.read_csv('all_yeast.csv',encoding = 'ISO-8859-1')\n",
    "\n",
    "#Checking the data\n",
    "#displaydata(yeast)\n",
    "#yeast.dtypes\n",
    "\n",
    "#Washing data \n",
    "#Drop the index-\"Sample\"\n",
    "yeast.drop([\"Sample\"], axis=1, inplace=True)\n",
    "\n",
    "#Convert the 'class' to 1 and 0. In case, there is also missing value in this column. We can Identify those column as sim-CYT \n",
    "mapping = {'non-CYT': 0, 'CYT': 1}\n",
    "yeast['Class'] = yeast['Class'].map(mapping)\n",
    "\n",
    "#Use the mean value or median value to fill the missing value \n",
    "yeastmea = yeast.fillna(yeast.mean())\n",
    "print(\"------- Display the min, median, max, mean and standard deviation for the data with Mean imputations -------\")\n",
    "displaydata(yeastmea)\n",
    "print(\"------- Display the min, median, max, mean and standard deviation for the data with Median imputations -------\")\n",
    "yeastmed = yeast.fillna(yeast.median())\n",
    "displaydata(yeastmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------- Mean centering -------\n",
      "Min:\n",
      "mcg     -0.387628\n",
      "gvh     -0.369643\n",
      "alm     -0.296167\n",
      "mit     -0.260432\n",
      "erl     -0.006739\n",
      "pox     -0.007500\n",
      "vac     -0.504199\n",
      "nuc     -0.276712\n",
      "Class    0.000000\n",
      "dtype: float64\n",
      "Median\n",
      "mcg     -0.017628\n",
      "gvh     -0.009643\n",
      "alm      0.003833\n",
      "mit     -0.040432\n",
      "erl     -0.006739\n",
      "pox     -0.007500\n",
      "vac      0.005801\n",
      "nuc     -0.056712\n",
      "Class    0.000000\n",
      "dtype: float64\n",
      "Max\n",
      "mcg      0.502372\n",
      "gvh      0.500357\n",
      "alm      6.995652\n",
      "mit      2.740296\n",
      "erl      2.994110\n",
      "pox      0.822500\n",
      "vac      5.497256\n",
      "nuc      4.224379\n",
      "Class    1.000000\n",
      "dtype: float64\n",
      "Mean\n",
      "mcg     -6.454855e-16\n",
      "gvh      7.581911e-16\n",
      "alm      6.118571e-16\n",
      "mit     -1.108353e-15\n",
      "erl      3.112215e-17\n",
      "pox      2.783390e-17\n",
      "vac      1.846307e-15\n",
      "nuc     -4.719364e-15\n",
      "Class    3.119946e-01\n",
      "dtype: float64\n",
      "Standard Deviation\n",
      "mcg      0.131472\n",
      "gvh      0.121954\n",
      "alm      0.199840\n",
      "mit      0.150817\n",
      "erl      0.091580\n",
      "pox      0.075683\n",
      "vac      0.153488\n",
      "nuc      0.152315\n",
      "Class    0.463464\n",
      "dtype: float64\n",
      " \n",
      "------- Standardisation -------\n",
      "Min:\n",
      "mcg     -2.948360\n",
      "gvh     -3.030990\n",
      "alm     -1.482023\n",
      "mit     -1.726808\n",
      "erl     -0.073587\n",
      "pox     -0.099098\n",
      "vac     -3.284942\n",
      "nuc     -1.816711\n",
      "Class    0.000000\n",
      "dtype: float64\n",
      "Median\n",
      "mcg     -0.134082\n",
      "gvh     -0.079069\n",
      "alm      0.019180\n",
      "mit     -0.268085\n",
      "erl     -0.073587\n",
      "pox     -0.099098\n",
      "vac      0.037794\n",
      "nuc     -0.372336\n",
      "Class    0.000000\n",
      "dtype: float64\n",
      "Max\n",
      "mcg       3.821121\n",
      "gvh       4.102818\n",
      "alm      35.006323\n",
      "mit      18.169692\n",
      "erl      32.694019\n",
      "pox      10.867746\n",
      "vac      35.815549\n",
      "nuc      27.734500\n",
      "Class     1.000000\n",
      "dtype: float64\n",
      "Mean\n",
      "mcg     -4.927475e-15\n",
      "gvh      6.224655e-15\n",
      "alm      3.127833e-15\n",
      "mit     -7.274879e-15\n",
      "erl      4.043636e-16\n",
      "pox     -4.006042e-16\n",
      "vac      1.214654e-14\n",
      "nuc     -3.019679e-14\n",
      "Class    3.119946e-01\n",
      "dtype: float64\n",
      "Standard Deviation\n",
      "mcg      1.000000\n",
      "gvh      1.000000\n",
      "alm      1.000000\n",
      "mit      1.000000\n",
      "erl      1.000000\n",
      "pox      1.000000\n",
      "vac      1.000000\n",
      "nuc      1.000000\n",
      "Class    0.463464\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Part2 Classification:\n",
    "#Pre-processing\n",
    "#Scale the features\n",
    "\n",
    "#Mean centering\n",
    "#Set a function for mean center \n",
    "def meancenter(data):\n",
    "    result = data - data.mean()\n",
    "    return result\n",
    "#Calculate the mean centering\n",
    "MC = meancenter(yeastmed[['mcg', 'gvh', 'alm', 'mit', 'erl', 'pox', 'vac', 'nuc']])\n",
    "MC['Class'] = yeastmed['Class']\n",
    "#print out\n",
    "print(\" \")\n",
    "print(\"------- Mean centering -------\")\n",
    "displaydata(MC)\n",
    "\n",
    "#Standardisation\n",
    "#Set a function for mean center \n",
    "def standsca(data):\n",
    "#Z-score scaling standardisation: variables recalculated as (V - mean of V)/s, where \"s\" is the standard deviation. \n",
    "#As a result, all variables in the data set have equal means (0) and standard deviations (1) but different ranges.\n",
    "    newdata = (data - data.mean()) / data.std()\n",
    "    return newdata\n",
    "#Apply the standardisation\n",
    "STD = standsca(yeastmed[['mcg', 'gvh', 'alm', 'mit', 'erl', 'pox', 'vac', 'nuc']])\n",
    "STD['Class'] = yeastmed['Class']\n",
    "#print out\n",
    "print(\" \")\n",
    "print(\"------- Standardisation -------\")\n",
    "displaydata(STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN = 5 accuracy: 0.6767676767676768\n",
      "KNN = 10 accuracy: 0.7070707070707071\n"
     ]
    }
   ],
   "source": [
    "#Part2 Classification:\n",
    "#Comparing Classification Algorithms\n",
    "\n",
    "#prepareation\n",
    "#Set and split x and y\n",
    "#x is all features\n",
    "x = MC.loc[:, \"mcg\":\"nuc\"]\n",
    "#y is CYT or NON CYT or the chance \n",
    "y = MC.loc[:, 'Class']\n",
    "\n",
    "#Split train set and test set\n",
    "#Use 2/3(0.66667) of data for training\n",
    "#Use 1/3(0.33333)  of data for testing\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.33333,random_state = 0)\n",
    "\n",
    "#KNN Algorithms\n",
    "#Set function for KNN\n",
    "def knnfunc(k):\n",
    "#Initialise the KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#Trainning\n",
    "    knn.fit(Xtrain, Ytrain)\n",
    "#Testing\n",
    "    result = knn.predict(Xtest)\n",
    "#Calculate the accuracy\n",
    "    predict = accuracy_score(Ytest, result)\n",
    "    print(\"KNN = \" + str(k) + \" accuracy: \" + str(predict))\n",
    "\n",
    "#Apply and print the KNN\n",
    "knnfunc(5)\n",
    "knnfunc(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy: 0.6828282828282828\n"
     ]
    }
   ],
   "source": [
    "#Part2 Classification:\n",
    "#Comparing Classification Algorithms\n",
    "\n",
    "#Decision tree Algorithms\n",
    "#Set function for Decision tree\n",
    "def decisiontree():\n",
    "#Initialise the vector and generate arry\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    xxtrain = vec.fit_transform(Xtrain.to_dict(orient='record'))\n",
    "    xxtest = vec.transform(Xtest.to_dict(orient='record'))\n",
    "#Initialise decision tree, and set the criterion to 'entropy' as it mentioned in the lecture\n",
    "    dt = DecisionTreeClassifier(criterion='entropy')\n",
    "#Trainning\n",
    "    dt.fit(xxtrain, Ytrain)\n",
    "#Predict\n",
    "    predict = dt.predict(xxtest)\n",
    "#Calculate the accuracy\n",
    "    accuracy = accuracy_score(Ytest, predict)\n",
    "    print('Decision tree accuracy: ' + str(accuracy))\n",
    "\n",
    "#Apply and print the Decision tree\n",
    "decisiontree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the graph above, we can see that 8 is the optimal value. \n"
     ]
    }
   ],
   "source": [
    "#part2 Classification:\n",
    "#Feature Engineering\n",
    "#Preparation\n",
    "\n",
    "#Choose the best K for Kmean\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "MC0 = MC.loc[:, \"mcg\":\"nuc\"]\n",
    "# creat a empty array\n",
    "Knum = []\n",
    "\n",
    "# put the data, which is the sum of squared distances of samples to their closest cluster centre, into the array.\n",
    "K = range(1,18)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, random_state=0)\n",
    "    km.fit(MC0)\n",
    "    Knum.append(km.inertia_)\n",
    "    \n",
    "# Plot the elbow plot\n",
    "plt.plot(K, Knum, 'o-')\n",
    "plt.title('The Elbow Method of optimal k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()\n",
    "\n",
    "print(\"According to the graph above, we can see that 8 is the optimal value. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- after feature generation(just Interaction term pairs) -------\n",
      "KNN = 5 accuracy: 0.7212121212121212\n",
      "KNN = 10 accuracy: 0.7111111111111111\n",
      " \n",
      "------- after feature generation(just Clustering labels) -------\n",
      "KNN = 5 accuracy: 0.6686868686868687\n",
      "KNN = 10 accuracy: 0.7070707070707071\n",
      " \n",
      "------- after feature generation(interaction term pairs and clustering labels) -------\n",
      "KNN = 5 accuracy: 0.7111111111111111\n",
      "KNN = 10 accuracy: 0.7131313131313132\n",
      " \n",
      "------- after feature selection -------\n",
      "KNN = 5 accuracy: 0.694949494949495\n",
      "KNN = 10 accuracy: 0.6747474747474748\n"
     ]
    }
   ],
   "source": [
    "#part2 Classification:\n",
    "#Feature Engineering\n",
    "\n",
    "#Feature generation\n",
    "\n",
    "#Just use interaction term pairs\n",
    "def featuregenerationInter():\n",
    "    #We can manipulate the varibles outside the function\n",
    "    global Xtrain, Xtest, Ytrain, Ytest\n",
    "    \n",
    "    #Remove the \"class\" firt\n",
    "    MC1= MC.loc[:, \"mcg\":\"nuc\"]\n",
    "    #Interaction term pairs as mention on the quesion and tried different combination\n",
    "    MC1['new_feature1'] = MC1['mcg'] * MC1['alm']\n",
    "    MC1['new_feature2'] = MC1['gvh'] / MC1['mit']\n",
    "    MC1['new_feature3'] = MC1['vac'] * MC1['nuc']\n",
    "#     MC1['new_feature4'] = MC1['pox'] / MC1['vac']\n",
    "#     MC1['new_feature5'] = MC1['mcg'] / MC1['mit']\n",
    "#     MC1['new_feature6'] = MC1['gvh'] * MC1['mit']\n",
    "#     MC1['new_feature7'] = MC1['erl'] / MC1['vac']\n",
    "    \n",
    "    #Change the 'class' as string\n",
    "    MC1['Class'] = yeast['Class']\n",
    "    \n",
    "    #Set and split x and y\n",
    "    #x is all features\n",
    "    x = MC1.loc[:, \"mcg\":\"new_feature3\"]\n",
    "    #y is CYT or NON CYT  \n",
    "    y = MC1.loc[:, 'Class']\n",
    "\n",
    "    #Split train set and test set\n",
    "    #Use 2/3(0.66667) of data for training\n",
    "    #Use 1/3(0.33333)  of data for testing\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.33333, random_state = 0)\n",
    "\n",
    "#Apply and print the Feature generation\n",
    "print(\"------- after feature generation(just Interaction term pairs) -------\") \n",
    "featuregenerationInter()\n",
    "knnfunc(5)\n",
    "knnfunc(10)\n",
    "print(\" \")     \n",
    "\n",
    "#Just use clustering labels\n",
    "def featuregenerationLa():\n",
    "    #We can manipulate the varibles outside the function\n",
    "    global Xtrain, Xtest, Ytrain, Ytest\n",
    "    \n",
    "    #Remove the \"class\" firt\n",
    "    MC2 = MC.loc[:, \"mcg\":\"nuc\"]\n",
    "    \n",
    "    #Use the Kmeans to generate new features as mention on the quesion\n",
    "    #I tried different k clusters. when the clusters = 2 we can get the best result \n",
    "    pred = KMeans(n_clusters=8, random_state=0).fit_predict(MC2)\n",
    "    MC2['new_feature0'] = pred\n",
    "\n",
    "    #Change the 'class' as string\n",
    "    MC2['Class'] = yeast['Class']\n",
    "    \n",
    "    #Set and split x and y\n",
    "    #x is all features\n",
    "    x = MC2.loc[:, \"mcg\":\"new_feature0\"]\n",
    "    #y is CYT or NON CYT  \n",
    "    y = MC2.loc[:, 'Class']\n",
    "\n",
    "    #Split train set and test set\n",
    "    #Use 2/3(0.66667) of data for training\n",
    "    #Use 1/3(0.33333)  of data for testing\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.33333, random_state = 0)\n",
    "\n",
    "#Apply and print the Feature generation\n",
    "print(\"------- after feature generation(just Clustering labels) -------\") \n",
    "featuregenerationLa()\n",
    "knnfunc(5)\n",
    "knnfunc(10)   \n",
    "print(\" \") \n",
    "\n",
    "#Use both interaction term pairs and clustering labels\n",
    "def featuregenerationall():\n",
    "    #We can manipulate the varibles outside the function\n",
    "    global Xtrain, Xtest, Ytrain, Ytest\n",
    "\n",
    "    #Remove the \"class\" feature for Kmeans, and after Feature generation, we put back\n",
    "    MC3 = MC.loc[:, \"mcg\":\"nuc\"]\n",
    "\n",
    "    #Use the Kmeans to generate new features as mention on the quesion\n",
    "    #I tried different k clusters. when the clusters = 2 we can get the best result \n",
    "    pred = KMeans(n_clusters=8, random_state=0).fit_predict(MC3)\n",
    "    MC3['new_feature0'] = pred\n",
    "\n",
    "    #Interaction term pairs as mention on the quesion and tried different combination\n",
    "    MC3['new_feature1'] = MC3['mcg'] * MC3['alm']\n",
    "    MC3['new_feature2'] = MC3['gvh'] / MC3['mit']\n",
    "    MC3['new_feature3'] = MC3['vac'] * MC3['nuc']\n",
    "#     MC3['new_feature4'] = MC3['pox'] / MC['vac']\n",
    "#     MC3['new_feature5'] = MC3['mcg'] / MC['mit']\n",
    "#     MC3['new_feature6'] = MC3['gvh'] * MC['mit']\n",
    "#     MC3['new_feature7'] = MC3['erl'] / MC['vac']\n",
    "    \n",
    "    #Change the 'class' as string\n",
    "    MC3['Class'] = yeast['Class']\n",
    "    \n",
    "    #Set and split x and y\n",
    "    #x is all features\n",
    "    x = MC3.loc[:, \"mcg\":\"new_feature3\"]\n",
    "    #y is CYT or NON CYT  \n",
    "    y = MC3.loc[:, 'Class']\n",
    "\n",
    "    #Split train set and test set\n",
    "    #Use 2/3(0.66667) of data for training\n",
    "    #Use 1/3(0.33333)  of data for testing\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.33333, random_state = 0)\n",
    "\n",
    "#Apply and print the Feature generation\n",
    "print(\"------- after feature generation(interaction term pairs and clustering labels) -------\") \n",
    "featuregenerationall()\n",
    "knnfunc(5)\n",
    "knnfunc(10)   \n",
    "print(\" \")     \n",
    "    \n",
    "#Selection\n",
    "def featureselect():\n",
    "    #We can manipulate the varibles outside the function\n",
    "    global Xtrain, Xtest, Ytrain, Ytest\n",
    "    \n",
    "    #Renew the 'Class'\n",
    "    MC4 = MC.loc[:, \"mcg\":\"nuc\"]\n",
    "    MC4['Class'] = yeast['Class']\n",
    "    \n",
    "    #Set and split x and y\n",
    "    #x is all features\n",
    "    x = MC4.loc[:, \"mcg\":\"nuc\"]\n",
    "    #y is CYT or NON CYT  \n",
    "    y = MC4.loc[:, 'Class']\n",
    "    \n",
    "    #We tried different K to see the result.\n",
    "    #After experiments, I selected 4 features with mutual information\n",
    "    x = SelectKBest(mutual_info_classif, k=4).fit_transform(x, y)\n",
    "\n",
    "    #Split train set and test set\n",
    "    #Use 2/3(0.66667) of data for training\n",
    "    #Use 1/3(0.33333)  of data for testing\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.33333, random_state = 0)\n",
    "\n",
    "#Apply and print the Selection\n",
    "print(\"------- after feature selection -------\")\n",
    "featureselect()\n",
    "knnfunc(5)\n",
    "knnfunc(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Feature selection+generation with clustering labels -------\n",
      "KNN = 5 accuracy: 0.701010101010101\n",
      "KNN = 10 accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "#part2 Classification:\n",
    "#Feature Engineering\n",
    "\n",
    "#Feature selection+generation with clustering labels \n",
    "\n",
    "def featuregNsL():\n",
    "    #We can manipulate the varibles outside the function\n",
    "    global Xtrain, Xtest, Ytrain, Ytest\n",
    "    \n",
    "    \n",
    "    #Remove the \"class\" feature for Kmeans, and after Feature generation, we put back\n",
    "    MC5  = MC.loc[:, \"mcg\":\"nuc\"]\n",
    "\n",
    "    #Use the Kmeans to generate new features as mention on the quesion\n",
    "    #I tried different k clusters. when the clusters = 2 we can get the best result \n",
    "    pred = KMeans(n_clusters=8, random_state=0).fit_predict(MC5)\n",
    "    MC5['new_feature0'] = pred\n",
    "    \n",
    "    #Change the 'class' as string\n",
    "    MC5['Class'] = yeast['Class']\n",
    "    \n",
    "    #Set and split x and y\n",
    "    #x is all features\n",
    "    x = MC5.loc[:, \"mcg\":\"new_feature0\"]\n",
    "    #y is CYT or NON CYT  \n",
    "    y = MC5.loc[:, 'Class']\n",
    "    \n",
    "    #We tried different K to see the result.\n",
    "    #After experiments, I selected 4 features with mutual information\n",
    "    x = SelectKBest(mutual_info_classif, k=4).fit_transform(x, y)\n",
    "\n",
    "    #Split train set and test set\n",
    "    #Use 2/3(0.66667) of data for training\n",
    "    #Use 1/3(0.33333)  of data for testing\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=0.33333, random_state = 0)\n",
    "\n",
    "#Apply and print the Feature generation\n",
    "print(\"------- Feature selection+generation with clustering labels -------\") \n",
    "featuregNsL()\n",
    "knnfunc(5)\n",
    "knnfunc(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
